{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Introduction\n",
    "\n",
    "We cover the following topics:\n",
    "\n",
    "- Tokenizing text using functions **word_tokenize** and **sent_tokenize**.\n",
    "\n",
    "- Computing Frequencies with **FreqDist** and **ConditionalFreqDist**.\n",
    "\n",
    "- Generating Bigrams and collocations with **bigrams** and **collocations**.\n",
    "\n",
    "- Stemming word affixes using **PorterStemmer** and **LancasterStemmer**.\n",
    "\n",
    "- Tagging words to their parts of speech using **pos_tag**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nltk\n",
    "\n",
    "**nltk** is a popular Python framework used for developing Python programs to work with human language data. Key features of nltk:\n",
    "\n",
    "- It provides access to over 50 text corpora and other lexical resources.\n",
    "- It is a suite of text processing tools.\n",
    "- It is free to use and Open source.\n",
    "- It is available for Windows, Mac OS X, and Linux.\n",
    "\n",
    "## Installing nltk\n",
    "\n",
    "nltk can be installed, automatically, using the command - \n",
    "\n",
    "***pip install nltk***\n",
    "and\n",
    "***nltk.download()***\n",
    "\n",
    "Installation can be verified by opening the Python terminal and typing command - \n",
    "***import nltk***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Understanding of nltk\n",
    "\n",
    "Now let's understand by performing simple tasks in the next couple of slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Splitting a sample text into a list of sentences using \"sent_tokenize\" function\n",
    "import nltk\n",
    "text = \"Python is an interpreted high-level programming language for general-purpose programming. \\\n",
    "Created by Guido van Rossum and first released in 1991.\"\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Python', 'is', 'an', 'interpreted', 'high-level']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Splitting a sample text into words using \"word_tokenize\" function\n",
    "words = nltk.word_tokenize(text)\n",
    "print(len(words))\n",
    "\n",
    "# The expression words[:5] displays first five words of list words.\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('programming', 2), ('.', 2)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Determining the frequency of words present in sample text using \"FreqDist\" function\n",
    "## The expression wordfreq.most_common(n) displays n highly frequent words with their respective frequency count.\n",
    "wordfreq = nltk.FreqDist(words)\n",
    "wordfreq.most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading NLTK Book collection\n",
    "\n",
    "In this notebook, you will be coordinating with several texts curated by NLTK authors. These texts are available in collection book of nltk. They can be downloaded by running the following command in Python interpreter, after importing nltk successfully.\n",
    "\n",
    "***nltk.download('book')***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "## The command loads nine texts and nine sentences, from the collection book\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching Text\n",
    "There are multiple ways of searching for a pattern in a text. The example shown below searches for words starting with tri, and ending with r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triangular; triangular; triangular; triangular\n"
     ]
    }
   ],
   "source": [
    "text1.findall(\"<tri.*r>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Tasks with Text\n",
    "In this topic, you will understand how to perform the following activities, using **text1** as input text.\n",
    "\n",
    "- Total Word Count\n",
    "- Unique Word Count\n",
    "- Transforming Words\n",
    "- Word Coverage\n",
    "- Filtering Words\n",
    "- Frequency Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Total Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.text.Text'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "260819"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The text1, imported from nltk.book is an object of nltk.text.Text class\n",
    "print(type(text1))\n",
    "\n",
    "## Total number of words in text1 is determined using len\n",
    "n_words = len(text1)\n",
    "n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Unique Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19317"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## A unique number of words in text1 is determined using set and len methods\n",
    "## The expression set(text1) generates list of unique words from text1\n",
    "n_unique_words = len(set(text1))\n",
    "n_unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Words\n",
    "It is possible to apply a function to any number of words and transform them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17231\n",
      "2086\n"
     ]
    }
   ],
   "source": [
    "## Now let's transform every word of text1 to lowercase and determine unique words once again.\n",
    "text1_lcw = [ word.lower() for word in set(text1) ]\n",
    "n_unique_words_lc = len(set(text1_lcw))\n",
    "print(n_unique_words_lc)\n",
    "print(abs(n_unique_words_lc - n_unique_words))\n",
    "## A difference of 2086 can be found from n_unique_words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Word Coverage\n",
    "***Word Coverage*** refers to an average number of times a word is occurring in the text. The following examples determine Word Coverage of raw and transformed text1 and also check coverage for all words when lowercased in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.502044830977896\n",
      "15.136614241773549\n"
     ]
    }
   ],
   "source": [
    "word_coverage1 = n_words / n_unique_words\n",
    "print(word_coverage1)\n",
    "word_coverage2 = n_words / n_unique_words_lc\n",
    "print(word_coverage2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Words\n",
    "Now let's see how to filter words based on specific criteria. The following example uses a list of comprehension with a condition and filters words having characters more than 17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uninterpenetratingly', 'characteristically']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_words = [word for word in set(text1) if len(word) > 17 ]\n",
    "big_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see one more example which filters words having the prefix ***Sun***. The example is case-sensitive. It doesn't filter the words starting with lowercase ***s*** and followed by ***un***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sunda', 'Sunday', 'Sunset']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sun_words = [word for word in set(text1) if word.startswith('Sun') ]\n",
    "sun_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Distribution\n",
    "***FreqDist*** functionality of nltk can be used to determine the frequency of all words, present in an input text.\n",
    "\n",
    "## Common Methods of Frequency Distribution\n",
    "Illustration of Commonly used methods on a frequency distribution fdist.\n",
    "![Frequency Distribution](files/Frequency_Distribution.jpeg \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The following example, determines frequency distribution of text1 and further displays the frequency of word Sunday.\n",
    "text1_freq = nltk.FreqDist(text1)\n",
    "text1_freq['Sunday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 18713), ('the', 13721), ('.', 6862)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now let's identify three frequent words from text1_freq distribution using most_common method.\n",
    "top3_text1 = text1_freq.most_common(3)\n",
    "top3_text1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "In general, you would be interested in finding frequent words which are not common in usage and specific to input text. In the next example, you will perform the following -\n",
    "\n",
    "- Filter words having all characters and of larger length.\n",
    "- Determine frequency distribution of the filtered words.\n",
    "- Identify the three most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Queequeg', 252), ('Starbuck', 196), ('something', 119)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_uncommon_words = [word for word in text1 if word.isalpha() and len(word) > 7 ]\n",
    "text1_uncommon_freq = nltk.FreqDist(large_uncommon_words)\n",
    "text1_uncommon_freq.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dictatorship']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.book import text6\n",
    "text6_freq = nltk.FreqDist(text6)\n",
    "text6_freq['BROTHER']\n",
    "big_words6 = [word for word in set(text6) if len(word) > 10 ]\n",
    "big_words6\n",
    "b = [word for word in set(text6) if word.endswith('ship') ]\n",
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.book import text6\n",
    "b = [word for word in text6 if word.endswith('ing') ]\n",
    "len(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular Text Corpora\n",
    "Two popular Text Corpora available from nltk, which you will be using are:\n",
    "\n",
    "- **Genesis**: It is a collection of few words across multiple languages.\n",
    "\n",
    "- **Brown**: It is the first electronic corpus of one million English words.\n",
    "\n",
    "### Other Corpus in nltk\n",
    "\n",
    "- **Gutenberg** : Collections from Project Gutenberg\n",
    "- **Inaugural** : Collection of U.S Presidents inaugural speeches\n",
    "- **stopwords** : Collection of stop words.\n",
    "- **reuters** : Collection of news articles.\n",
    "- **cmudict** : Collection of CMU Dictionary words.\n",
    "- **movie_reviews** : Collection of Movie Reviews.\n",
    "- **np_chat** : Collection of chat text.\n",
    "- **names** : Collection of names associated with males and females.\n",
    "- **state_union** : Collection of state union address.\n",
    "- **wordnet** : Collection of all lexical entries.\n",
    "- **words** : Collection of words in Wordlist corpus.\n",
    "\n",
    "### Accessing Text Corpora\n",
    "Any text corpus has to be imported before you start working with it. The below code imports genesis text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['english-kjv.txt',\n",
       " 'english-web.txt',\n",
       " 'finnish.txt',\n",
       " 'french.txt',\n",
       " 'german.txt',\n",
       " 'lolcat.txt',\n",
       " 'portuguese.txt',\n",
       " 'swedish.txt']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import genesis\n",
    "## Various text collections available under genesis text corpus are viewed by fileids method.\n",
    "genesis.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with a Text Corpus\n",
    "Now let's understand how to work with a text corpus. The following example determines the average word length and average sentence length of each text collection present in genesis corpus.\n",
    "\n",
    "The methods raw, words and sents used in code determine the total number of characters, words, and sentences present in a specific text collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 30 english-kjv.txt\n",
      "4 19 english-web.txt\n",
      "5 15 finnish.txt\n",
      "4 23 french.txt\n",
      "4 23 german.txt\n",
      "4 20 lolcat.txt\n",
      "4 27 portuguese.txt\n",
      "4 30 swedish.txt\n"
     ]
    }
   ],
   "source": [
    "for fileid in genesis.fileids():\n",
    "    n_chars = len(genesis.raw(fileid))\n",
    "    n_words = len(genesis.words(fileid))\n",
    "    n_sents = len(genesis.sents(fileid))\n",
    "    print(int(n_chars/n_words), int(n_words/n_sents), fileid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Corpus Structure\n",
    "A text corpus is organized into any of the following four structures.\n",
    "\n",
    "- Isolated - Holds Individual text collections.\n",
    "- Categorized - Each text collection tagged to a category.\n",
    "- Overlapping - Each text collection tagged to one or more categories, and\n",
    "- Temporal - Each text collection tagged to a period, date, time, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading User Specific Corpus\n",
    "Now let's see how to convert your collection of text files into a text corpus. Suppose, you have three files c1.txt, c2.txt and c3.txt in current directory path.\n",
    "\n",
    "Creation of corpus wordlists corpus is shown in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints/NLP-checkpoint.ipynb',\n",
       " 'Frequency_Distribution.jpeg',\n",
       " 'NLP.ipynb',\n",
       " 'c1.txt',\n",
       " 'c2.txt',\n",
       " 'c3.txt',\n",
       " 'cfdist.jpeg']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import os \n",
    "corpus_root = os.getcwd()\n",
    "\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
    "wordlists.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PlaintextCorpusReader in 'C:\\\\Users\\\\svksh\\\\Documents\\\\#Personal\\\\Fresco\\\\Machine Learning\\\\NLP using Python'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Frequency\n",
    "In the previous topic, you have studied about Frequency Distributions. ***FreqDist*** function computes the frequency of each item in a list. While computing a frequency distribution, you observe occurrence count of an event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'apple': 2, 'cabbage': 2, 'kiwi': 1, 'potato': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = ['apple', 'apple', 'kiwi', 'cabbage', 'cabbage', 'potato']\n",
    "nltk.FreqDist(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Conditional Frequency\n",
    "A Conditional Frequency is a collection of frequency distributions, computed based on a condition. For computing a conditional frequency, you have to attach a condition to every occurrence of an event. Let's consider the following list for computing Conditional Frequency.\n",
    "\n",
    "***ConditionalFreqDist*** function of nltk is used to compute Conditional Frequency Distribution (CDF). The same can be viewed in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F', 'V']\n",
      "<FreqDist with 2 samples and 3 outcomes>\n",
      "<FreqDist with 2 samples and 3 outcomes>\n"
     ]
    }
   ],
   "source": [
    "c_items = [('F','apple'), ('F','apple'), ('F','kiwi'), ('V','cabbage'), ('V','cabbage'), ('V','potato') ]\n",
    "cfd = nltk.ConditionalFreqDist(c_items)\n",
    "print(cfd.conditions())\n",
    "print(cfd['V'])\n",
    "print(cfd['F'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common methods of a Conditional Frequency\n",
    "Illustration of Commonly used methods on a conditional frequency distribution, cfdist.\n",
    "\n",
    "![CFD](files/cfdist.jpeg \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Words by Genre\n",
    "Now let's determine the frequency of words, of a particular genre, in brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "cfd = nltk.ConditionalFreqDist([ (genre, word) for genre in brown.categories() for word in brown.words(categories=genre) ])\n",
    "cfd.conditions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Word Count\n",
    "Once after computing conditional frequency distribution, tabulate method is used for viewing the count along with arguments conditions and samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           leadership    worship   hardship \n",
      "government         12          3          2 \n",
      "     humor          1          0          0 \n",
      "   reviews         14          1          2 \n"
     ]
    }
   ],
   "source": [
    "cfd.tabulate(conditions=['government', 'humor', 'reviews'], samples=['leadership', 'worship', 'hardship'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Cumulative Word Count\n",
    "The cumulative count for different conditions is found by setting cumulative argument value to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           leadership    worship   hardship \n",
      "government         12         15         17 \n",
      "     humor          1          1          1 \n",
      "   reviews         14         15         17 \n"
     ]
    }
   ],
   "source": [
    "cfd.tabulate(conditions=['government', 'humor', 'reviews'], samples=['leadership', 'worship', 'hardship'], cumulative = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Individual Frequency Distributions\n",
    "From the obtained conditional frequency distribution, you can access individual frequency distributions. The below example extracts frequency distribution of words present in news genre of brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_fd = cfd['news']\n",
    "news_fd.most_common(3)\n",
    "news_fd['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Frequency Distributions\n",
    "Now let's see another example, which computes the frequency of last character appearing in all names associated with males and females respectively and compares them. The text corpus names contain two files male.txt and female.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "          a    e \n",
      "female 1773 1432 \n",
      "  male   29  468 \n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import names\n",
    "nt = [(fid.split('.')[0], name[-1]) for fid in names.fileids() for name in names.words(fid) ]\n",
    "cfd2 = nltk.ConditionalFreqDist(nt)\n",
    "\n",
    "## The expression cfd2['female'] > cfd2['male'] checks if the last characters in females occur more frequently than the\n",
    "## last characters in males.\n",
    "print(cfd2['female'] > cfd2['male'])\n",
    "\n",
    "## The following code snippet displays frequency count of characters a and e in females and males, respectively.\n",
    "cfd2.tabulate(samples=['a', 'e'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Text Processing\n",
    "For most of the NLTK studies that you carry out, data is not readily available in the form of a text corpus. Also, raw text data from a different source can be obtained, processed and used for doing NLTK studies.\n",
    "\n",
    "Some of the processing steps that you perform are :\n",
    "- Tokenization\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a Text File\n",
    "In this topic, you will understand how data is read from different external sources. The following example reads content from a text file, available at Project Gutenberg site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "content1 = request.urlopen(url).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a HTML file\n",
    "The following example reads content from a news article available over the web. ***Beautifulsoup*** module is used for scrapping the required text from the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "url = \"http://www.bbc.com/news/health-42802191\"\n",
    "html_content = request.urlopen(url).read()\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***find_all*** method returns all inner elements of div element, having class attribute value as ***story-body__inner***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_body = soup.find_all('div', attrs={'class':'story-body__inner'})\n",
    "inner_text = [elm.text for elm in inner_body[0].find_all(['h1', 'h2', 'p', 'li']) ]\n",
    "text_content2 = '\\n'.join(inner_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from Other Sources\n",
    "You can also read text from some other text resources such as RSS feeds, FTP repositories, local text files, etc.\n",
    "It is also possible to read a text in binary format, from sources like Microsoft Word and PDF.\n",
    "Third party libraries such as ***pywin32***, ***pypdf*** are required for accessing Microsoft Word or PDF documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "***Tokenization*** is a step in which a text is broken down into words and punctuation. The simplest way of tokenizing is by using ***word_tokenize*** method. The below example tokenizes text read from Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project', 'Gutenberg', 'EBook', 'of', 'Crime']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_content1 = content1.decode('unicode_escape')  # Converts bytes to unicode\n",
    "tokens1 = nltk.word_tokenize(text_content1)\n",
    "tokens1[3:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example tokenizes text scrapped from the HTML page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Smokers', 'need', 'to', 'quit', 'cigarettes']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "751"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2 = nltk.word_tokenize(text_content2)\n",
    "print(tokens2[:5])\n",
    "len(tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions for Tokenization\n",
    "Regular expressions can also be utilized to split the text into tokens using ***re*** module in python. The below example splits the entire text text_content2 with regular expression **\\w+**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "668"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "tokens2_2 = re.findall(r'\\w+', text_content2)\n",
    "len(tokens2_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of NLTK text\n",
    "Using the obtained list of tokens, an object of NLTK text can be created as shown below. This obtained text can be used for further linguistic processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text2 = nltk.Text(tokens2)\n",
    "type(input_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python is cool!!', '!']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Python is cool!!!'\n",
    "nltk.sent_tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams\n",
    "Bigrams represent a set of two consecutive words appearing in a text.\n",
    "***bigrams*** function is called on tokenized words, as shown in the following example, to obtain bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'is'),\n",
       " ('is', 'an'),\n",
       " ('an', 'awesome'),\n",
       " ('awesome', 'language'),\n",
       " ('language', '.')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "s = 'Python is an awesome language.'\n",
    "tokens = nltk.word_tokenize(s)\n",
    "list(nltk.bigrams(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Frequent Bigrams\n",
    "Now let's find out three frequently occurring bigrams, present in ***english-kjv*** collection of ***genesis*** corpus. Let's consider only those bigrams, whose words are having a length greater than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import genesis\n",
    "eng_tokens = genesis.words('english-kjv.txt')\n",
    "eng_bigrams = nltk.bigrams(eng_tokens)\n",
    "filtered_bigrams = [ (w1, w2) for w1, w2 in eng_bigrams if len(w1) >=5 and len(w2) >= 5 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After computing bi-grams, the following code computes frequency distribution and displays three most frequent bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('their', 'father'), 19), (('lived', 'after'), 16), (('seven', 'years'), 15)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_bifreq = nltk.FreqDist(filtered_bigrams)\n",
    "eng_bifreq.most_common(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Frequent After Words\n",
    "Now let's see an example which determines the two most frequent words occurring after living are determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('creature', 7), ('thing', 4)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import genesis\n",
    "eng_tokens = genesis.words('english-kjv.txt')\n",
    "eng_bigrams = nltk.bigrams(eng_tokens)\n",
    "eng_cfd = nltk.ConditionalFreqDist(eng_bigrams)\n",
    "eng_cfd['living'].most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Frequent Next Word\n",
    "Now let's define a function named generate, which returns words occurring frequently after a given word.\n",
    "After defining the function generate, it is called with ***eng_cfd*** and ***living*** parameters.\n",
    "The output shows a word which occurs most frequently next to ***living*** is ***creature***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['living', 'creature', 'that', 'he', 'said']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(cfd, word, n=5):\n",
    "    n_words = []\n",
    "    for i in range(n):\n",
    "        n_words.append(word)\n",
    "        word = cfd[word].max()\n",
    "    return n_words\n",
    "\n",
    "generate(eng_cfd, 'living')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigrams\n",
    "Similar to Bigrams, Trigrams refers to set of all three consecutive words appearing in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'is', 'an'),\n",
       " ('is', 'an', 'awesome'),\n",
       " ('an', 'awesome', 'language'),\n",
       " ('awesome', 'language', '.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Python is an awesome language.'\n",
    "tokens = nltk.word_tokenize(s)\n",
    "list(nltk.trigrams(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ngrams\n",
    "***nltk*** also provides the function **ngrams**. It can be used to determine a set of all possible n consecutive words appearing in a text.\n",
    "\n",
    "The following example displays a list of four consecutive words appearing in the text ***s***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'is', 'an', 'awesome'),\n",
       " ('is', 'an', 'awesome', 'language'),\n",
       " ('an', 'awesome', 'language', '.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.ngrams(tokens, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations\n",
    "A collocation is a pair of words that occur together, very often.\n",
    "\n",
    "For example, red wine is a collocation.\n",
    "\n",
    "One characteristic of a collocation is that the words in it cannot be substituted with words having similar senses.\n",
    "\n",
    "For example, the combination maroon wine sounds odd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Collocations\n",
    "Now let's see how to generate collocations from text with the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said unto; pray thee; thou shalt; thou hast; thy seed; years old;\n",
      "spake unto; thou art; LORD God; every living; God hath; begat sons;\n",
      "seven years; shalt thou; little ones; living creature; creeping thing;\n",
      "savoury meat; thirty years; every beast\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import genesis\n",
    "tokens = genesis.words('english-kjv.txt')\n",
    "gen_text = nltk.Text(tokens)\n",
    "gen_text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Stemming is a process of stripping affixes from words.\n",
    "\n",
    "More often, you normalize text by converting all the words into lowercase. This will treat both words **The** and **the** as same. With stemming, the words **playing, played and play** will be treated as single word, i.e. **play**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemmers in nltk\n",
    "**nltk** comes with few stemmers. The two widely used stemmers are ***Porter*** and ***Lancaster*** stemmers. These stemmers have their own rules for string affixes.\n",
    "\n",
    "The following example demonstrates stemming of word ***builders*** using ***PorterStemmer***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'builder'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import PorterStemmer\n",
    "porter = nltk.PorterStemmer()\n",
    "porter.stem('builders')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how to use ***LancasterStemmer*** and stem the word builders. Lancaster Stemmer returns build whereas Porter Stemmer returns builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'build'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import LancasterStemmer\n",
    "lancaster = LancasterStemmer()\n",
    "lancaster.stem('builders')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing with Stemming\n",
    "Let's consider the text collection, text1.\n",
    "\n",
    "Let's first determine the number of unique words present in original text1. Then normalize the text by converting all the words into lower case and again determine the number of unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
      "19317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17231"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "print(len(set(text1)))\n",
    "lc_words = [ word.lower() for word in text1] \n",
    "len(set(lc_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's further normalize text1 with ***Porter*** Stemmer.\n",
    "The above output shows that, after normalising with Porter Stemmer, the text1 collection has 10927 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10927"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "p_stem_words = [porter.stem(word) for word in set(lc_words) ]\n",
    "len(set(p_stem_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's normalise with ***Lancaster*** stemmer and determine the unique words of text1. Applying Lancaster Stemmer to text1 collection resulted in 9036 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9036"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import LancasterStemmer\n",
    "lancaster = LancasterStemmer()\n",
    "l_stem_words = [lancaster.stem(word) for word in set(lc_words) ]\n",
    "len(set(l_stem_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Lemma\n",
    "**Lemma** is a lexical entry in a lexical resource such as word dictionary. You can find multiple Lemma's with the same spelling. These are known as **homonyms**.\n",
    "\n",
    "For example, consider the two Lemma's listed below, which are **homonyms**.\n",
    "1. saw [verb] - Past tense of see\n",
    "2. saw [noun] - Cutting instrument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "**nltk** comes with **WordNetLemmatizer**. This lemmatizer removes affixes only if the resulting word is found in lexical resource, **Wordnet**.\n",
    "\n",
    "**WordNetLemmatizer** is majorly used to build a vocabulary of words, which are valid Lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15168"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "wnl_stem_words = [wnl.lemmatize(word) for word in set(lc_words) ]\n",
    "len(set(wnl_stem_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "The method of categorizing words into their parts of speech and then labeling them respectively is called **POS Tagging**.\n",
    "\n",
    "### POS Tagger\n",
    "A POS Tagger processes a sequence of words and tags a part of speech to each word.\n",
    "\n",
    "***pos_tag*** is the simplest tagger available in nltk. The below example shows usage of ***pos_tag***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'NNP'), ('is', 'VBZ'), ('awesome', 'JJ'), ('.', '.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Python is awesome.'\n",
    "words = nltk.word_tokenize(text)\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words ***Python***, ***is*** and ***awesome*** are tagged to Proper Noun (NNP), Present Tense Verb (VB), and adjective (JJ) respectively. You can read more about the pos tags with the below help command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()\n",
    "## To know about a specific tag like JJ, use the below-shown expression\n",
    "nltk.help.upenn_tagset('JJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging Text\n",
    "Constructing a list of tagged words from a string is possible. A tagged word or token is represented in a tuple, having the word and the tag. In the input text, each word and tag are separated by **/**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'NN'), ('is', 'VB'), ('awesome', 'JJ'), ('.', '.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Python/NN is/VB awesome/JJ ./.'\n",
    "[ nltk.tag.str2tuple(word) for word in text.split() ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagged Corpora\n",
    "Many of the text corpus available in nltk, are already tagged to their respective parts of speech.\n",
    "\n",
    "***tagged_words*** method can be used to obtain tagged words of a corpus. The following example fetches tagged words of brown corpus and displays few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_tagged = brown.tagged_words()\n",
    "brown_tagged[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DefaultTagger\n",
    "***DefaultTagger*** assigns a specified tag to every word or token of given text. An example of tagging **NN** tag to all words of a sentence, is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'NN'), ('is', 'NN'), ('awesome', 'NN'), ('.', 'NN')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Python is awesome.'\n",
    "words = nltk.word_tokenize(text)\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "default_tagger.tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup Tagger\n",
    "You can define a custom tagger and use it to tag words present in any text. The below-shown example defines a dictionary defined_tags, with three words and their respective tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "text = 'Python is awesome.'\n",
    "words = nltk.word_tokenize(text)\n",
    "defined_tags = {'is':'BEZ', 'over':'IN', 'who': 'WPS'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Tagger\n",
    "***UnigramTagger*** provides you the flexibility to create your taggers. Unigram taggers are built based on statistical information. i.e., they tag each word or token to most likely tag for that particular word.\n",
    "\n",
    "You can build a unigram tagger through a process known as **training**.\n",
    "\n",
    "Then use the tagger to tag words in a test set and evaluate the performance.\n",
    "The example further defines a **UnigramTagger** with the defined dictionary and uses it to predict tags of words in text.\n",
    "Since the words Python and awesome are not found in **defined_tags** dictionary, they are tagged to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', None), ('is', 'BEZ'), ('awesome', None), ('.', None)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_tagger = nltk.UnigramTagger(model=defined_tags)\n",
    "baseline_tagger.tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the tagged sentences of brown corpus collections, associated with government genre. Let's also compute the training set size, i.e., 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2425"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_tagged_sents = brown.tagged_sents(categories='government')\n",
    "brown_sents = brown.sents(categories='government')\n",
    "len(brown_sents)\n",
    "train_size = int(len(brown_sents)*0.8)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***unigram_tagger*** is built by passing trained tagged sentences as argument to ***UnigramTagger***.\n",
    "\n",
    "The built unigram_tagger is further evaluated with test sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7799495586380832"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = brown_tagged_sents[:train_size]\n",
    "test_sents = brown_tagged_sents[train_size:]\n",
    "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
    "unigram_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('first', 'OD'),\n",
       " ('step', 'NN'),\n",
       " ('is', 'BEZ'),\n",
       " ('a', 'AT'),\n",
       " ('comprehensive', 'JJ'),\n",
       " ('self', None),\n",
       " ('study', 'NN'),\n",
       " ('made', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('faculty', None),\n",
       " (',', ','),\n",
       " ('by', 'IN'),\n",
       " ('outside', 'IN'),\n",
       " ('consultants', 'NNS'),\n",
       " (',', ','),\n",
       " ('or', 'CC'),\n",
       " ('by', 'IN'),\n",
       " ('a', 'AT'),\n",
       " ('combination', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('two', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The following code snippet shows tagging words of a sentence, taken from the test set.\n",
    "unigram_tagger.tag(brown_sents[3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary on NLP with Python\n",
    "\n",
    "- Tokenizing text using functions word_tokenize and sent_tokenize.\n",
    "\n",
    "- Computing Frequencies with FreqDist and ConditionalFreqDist.\n",
    "\n",
    "- Generating Bigrams and collocations with bigrams and collocations.\n",
    "\n",
    "- Stemming word affixes using PorterStemmer and LancasterStemmer.\n",
    "\n",
    "- Tagging words to their parts of speech using pos_tag."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
